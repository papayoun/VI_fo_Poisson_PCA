---
title: "About Variational Bayes for probabilistic PCA"
author: "Eric PARENT & Pierre Gloaguen"
date: "the 21st of April 2022 , UMR MIA-Paris, AgroParisTech, INRA, Universit√© Paris-Saclay"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    code_folding: hide
    df_print: paged
    self_contained: yes
    number_sections: yes
    toc: yes
    theme: journal
    toc_float: yes
    toc_depth: 2
    highlight: tango
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE,
                      message = FALSE, warning = FALSE)
```

# R codes and librairies {-}

All `R` codes can be unfolded, for instance, unfold the following chunk to see which libairies are used in this document:

```{r librairies, cache = FALSE}
rm(list=ls()) # Cleaning environment
library(tidyverse) # For data manipulation and visualization
library(rjags)  # For MCMC sampling
library(ggmcmc) # To format jags output
library(extraDistr) # For the non standart t distribution
```


# A simple latent factor model

## Model

We consider $n$ independant $p$-dimensional vectors $Y_i (1\leq i \leq n)$ such that, for every $i$:

$$Y_i = \Lambda \eta_i + E_i,$$

where $\eta_i$ is a $k$-dimensional independant standard Gaussian vector, $\eta_i \sim \mathcal{N}_k\left(0, I_k\right)$, Lambda is a $p\times k$ matrix of unknown real values, and $E_i$ is $p$-dimensional Gaussian vector with diagonal variance matrix, $E_i\sim \mathcal{N}_p(0, \Sigma)$, where $\Sigma = \text{diag}\left(\sigma^2_j\right)_{1\leq j \leq p}$.

```{r source_generating_data, include = FALSE}
source("utils_generating_data_PPCA.R")
```

```{r show_generating_data, file='utils_generating_data_PPCA.R'}
```

## Priors

### Prior for $\Sigma$ {-}

For each $1\leq j \leq p$, we consider 
$$\sigma^{-2}_j \sim \mathcal{G}(a^{\sigma}, b^\sigma),$$ where $\nu$ is an unknwon prior parameter

### Prior for $\Lambda$

We first consider a sequence $(\delta_h)_{1\leq h \leq k}$ of random variables such that:
\begin{align*}
\delta_1 &\sim \mathcal{G}(a_1, 1)\\
\delta_h &\sim \mathcal{G}(a_h, 1),~2\leq h \leq k.
\end{align*}
From this sequence, we define the sequence $(\tau_h)_{1\leq h \leq k}$ such that:
$$\tau_h = \prod_{\ell = 1}^h \delta_\ell, 1\leq h \leq k.$$
Moreover, consider, for $1\leq j \leq p$ and $1\leq h\leq k$ the random variables 
$$\phi_{j,h} \sim \mathcal{G}(\frac{\nu}{2}, \frac{\nu}{2}),$$
then, we define the prior over $\lambda_{j,h}$ as 
$$\lambda_{j,h} \sim \mathcal{N}\left(0, (\phi_{j,h}\tau_h)^{-1}\right).$$

# Variational inference

## Variational family

We propose to approximate the posterior distribution of all unknown variables by the following mean-field family:
$$q(\Lambda, \eta, \Sigma, \phi, \delta) = \prod_{j,h = 1}^{p,k}q(\lambda_{j,h})q(\phi_{j,h})\prod_{i = 1}^{n}q(\eta_i)\prod_{j = 1}^pq(\sigma^{-2}_j)\prod_{h = 1}^{k}q(\delta_h).$$
Moreover, we suppose that:
\begin{align*}
q(\lambda_{j,h}) &\text{ is the p.d.f. of a } \mathcal{N}(\mu_{j,h}^\lambda, s_{j,h}^\lambda)\\
q(\eta_{i}) &\text{ is the p.d.f. of a } \mathcal{N}(\mu_{i}^\eta, \Sigma_{i}^\eta)\\
q(\phi_{j,h}) &\text{ is the p.d.f. of a } \mathcal{G}(a_{j,h}^\phi, b_{j,h}^\phi)\\
q(\delta_{h}) &\text{ is the p.d.f. of a } \mathcal{G}(a_{h}^\delta, b_{h}^\delta) \\
q(\sigma^{-2}_{j}) &\text{ is the p.d.f. of a } \mathcal{G}(a_{j}^\sigma, b_{j}^\sigma) \\
\end{align*}

## Log joint distribution

We use the following decomposition:

\begin{align*}
\log \left[Y, \Lambda, \eta, \Sigma^{-1}, \phi, \delta\right] =& \sum_{i = 1}^n \log \left[Y_i \vert \Lambda, \eta, \Sigma\right] \\
&+ \log \left[\Lambda \vert \phi, \delta\right] \\
&+ \sum_{i = 1}^n \log \left[\eta_i\right]\\
&+ \sum_{j = 1}^p\log \left[\sigma^{-2}_j\right]\\
&+ \sum_{j = 1}^p\sum_{h = 1}^k\log\left[\phi_{j,h}\right]\\
&+ \sum_{h = 1}^k\log\left[\tau_{h}\right]
\end{align*}

## Updates

Following the CAVI algorithm, each parameter $\theta$ distribution is updated sequentially by computing 
$$\mathbb{E}_{-\theta}\left[\log \left[Y, \Lambda, \eta, \Sigma^{-1}, \phi, \delta\right]\right],$$
where $\mathbb{E}_{-\theta}[\cdot]$ denotes the expectation with respect to all random variables except $\theta$.

### Update $\eta_i$

In the log joint distribution, the only terms involving $\eta_i$ are given by

$$
f(\eta_i; \Lambda, \Sigma) = -\frac{1}{2}\left(
(Y_i - \Lambda\eta_i)^T\Sigma^{-1}(Y_i - \Lambda\eta_i) + \eta_i^T \eta_i
\right)
=
-\frac{1}{2}\left(
\eta_i^T(\overset{:=S}{\Lambda^T\Sigma^{-1}\Lambda} + I_k) \eta_i -2\eta_i^T\Lambda^T\Sigma^{-1}Y_i + \text{Cst}
\right).
$$
One can easily show that $1\leq j_1,j_2\leq p$
$$S_{j_1,j_2} = \sum_{\ell=1}^{p}\sigma^{-2}_l \lambda_{\ell,j_1}\lambda_{\ell, j_2}.$$
Therefore, taking expectations of both terms with respect to $\Lambda$ and $\Sigma$ (and using independance assumption), we have that

$$
\mathbb{E}_{\Lambda, \Sigma}\left[f(\eta_i; \Lambda, \Sigma)\right] =
-\frac{1}{2}\left(
\eta_i^T\left(\mathbb{E}\left[S\right] + I_k\right) \eta_i -2\eta_i^T\mathbb{E}\left[\Lambda\right]^T\mathbb{E}\left[\Sigma^{-1}\right]Y_i + \text{Cst}
\right),
$$
where:
\begin{align}
\mathbb{E}[S]_{j_1,j_2} &= 
\left\lbrace 
\begin{array}{lr}
\sum_{\ell=1}^{p}\frac{a_\ell^\sigma}{b_\ell^{\sigma}} \mu^\lambda_{\ell,j_1}\mu^\lambda_{\ell, j_2} & \text{ si } j_1\neq j_2\\
\sum_{\ell=1}^{p}\frac{a_\ell^\sigma}{b_\ell^{\sigma}} \left(\left(\mu^\lambda_{\ell,j_1}\right)^2 + s^\lambda_{\ell,j_1}\right) & \text{ si } j_1 = j_2
\end{array}
\right.\\
\mathbb{E}\left[\Lambda\right]_{j,h} &= \mu^\lambda_{j, h}\\
\mathbb{E}\left[\Sigma^{-1}\right] &= \text{diag}\left(\frac{a_j^\sigma}{b_j^{\sigma}}\right)_{1\leq j \leq p}
\end{align}

### Update $\Lambda_{j,-}$

One can easily rewrite the model in a column wise formulation, denoting $Y^j$ the $j$-th column of $Y$, $\Lambda_{j,-}$ the $j$-th row of $\Lambda$ (seen as a $k\times 1$ vector), and $\eta$ the $n \times k$ matrix stacking rowwise all $(\eta_i)_{1\leq i \leq n}$, we have:
$$Y^j = \eta \Lambda_{j, -} + E_j,$$
with $E_j \sim \mathcal{N}_k(0, \sigma^2_jI_k)$.
Therefore, terms involving $\Lambda$ in the log joint distribution can be rewritten as:
\begin{align*}
&f(\Lambda;\eta;\Sigma; \phi; \delta) &= \sum_{j = 1}^p \log \left[Y_j \vert \Lambda_{j,-}, \eta, \Sigma\right] + \sum_{j = 1}^p \log \left[\Lambda_{j,-} \vert \phi, \delta\right]\\
\Rightarrow & f(\Lambda_{j, -} &= -\frac{1}{2}
\left(
\sigma^{-2}_j\left(Y_j - \eta\Lambda_{j,-}\right)^T\left(Y_j - \eta\Lambda_{j,-}\right) + 
\Lambda_{j,-}^T D_j^{-1} \Lambda_{j, -}
\right),
\end{align*}
where $D_j^{-1} = \text{diag}\left(\phi_{j,h}\tau_j\right)_{1\leq h \leq k}$.
Thus, as previously, one can easily identify the moment of the underlying multivariate Gaussian

### Update $\sigma^{-2}_{j}$

The difficulty here is to obtain $\mathbb{E}[\Lambda_{j,-}^T\eta^T\eta\Lambda_{j, -}]$.
We write that 
$$\Lambda_{j,-}^T\eta^T\eta\Lambda_{j, -} = \sum_{i=1}^n (\Lambda_{j,-}^T\eta_i)^2 = \sum_{i = 1}^n\sum_{h_1, h_2 = 1}^k\lambda_{j,h_1}\lambda_{j,h_2}\eta_{i,h_1}\eta_{i,h_2},$$
whose expectation termwise can be obtained.

$$
B^{\sigma_j} = \text{tr}\left[
\sum_{i = n}\left(\mu^{\eta_i}(\mu^{\eta_i})^T + S^{\eta_i}\right)
\left(\mu^{\Lambda_{j, -}}(\mu^{\Lambda_{j, -}})^T + S^{\eta_i}\right)
\right]
$$

### Update $\phi$

$$
A^{\phi_{j,h}} = \frac{\nu + 1}{2}; B^{\phi_{j,h}} = \frac{\nu}{2} + \frac{1}{2}\prod_{\ell = 1}^h \frac{A^{\delta_\ell}}{B^{\delta_{\ell}}}
$$
