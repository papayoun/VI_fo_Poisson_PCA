---
title: "About Variational Bayes for PPCA data"
author: "Eric PARENT & Pierre Gloaguen"
date: "the 30th of Mai 2022 , UMR MIA-Paris, AgroParisTech, INRA, Universit√© Paris-Saclay"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    code_folding: hide
    df_print: paged
    self_contained: yes
    number_sections: yes
    toc: yes
    theme: journal
    toc_float: yes
    toc_depth: 2
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = FALSE}

# knitr::opts_chunk$set(
# 	echo = TRUE,
# 	message = FALSE,
# 	warning = FALSE,
# 	cache = FALSE
# )
knitr::opts_chunk$set(
	eval = FALSE,
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	include = TRUE
)
```

# R codes and librairies {-}

All `R` codes can be unfolded, for instance, unfold the following chunk to see which libairies are used in this document:

```{r librairies}
rm(list=ls())
library(tidyverse) # For data manipulation and visualization
library(rjags)  # For MCMC sampling
library(ggmcmc) # To format jags output
library(extraDistr) # For the non standart t distribution
```


# A not-so-simple shrinkage PPCA model

Bhattacharya and Dunson consider the following linear normal model with a gamma process to shrink the variances of the columns of the latent matrix $\lambda$:
\begin{align*}
Y_i &
\overset{ind.}{\sim} \mathcal{N}_p(\Lambda\eta_i, \Sigma^{-1}), &
1\leq i \leq n 
\\
\Sigma &= 
\text{diag}(\sigma_j^{-2}), &
1\leq j \leq p 
\\
\sigma_j^{-2}|(a_{\sigma},b_{\sigma}) &
\sim \Gamma(a_{\sigma},b_{\sigma}) &
1\leq j \leq p
\\
\Lambda_{jh} &
\sim \mathcal{N}_{q}(0,\,\phi_{jh}^{-1}\tau_{h}^{-1}), &
1\leq j \leq p, 1 \leq h \leq {q}
\\
\phi_{jh} &
\sim \Gamma(\frac{\nu}{2},\frac{\nu}{2}), &
1\leq j \leq p, 1 \leq h \leq {q}\\
\tau_{h} &
= \prod_{l=1}^h \delta_l &
1\leq h \leq q
\\ 
\delta_1 &
\sim \Gamma(a_1,1) &
\\
\delta_h &
\sim \Gamma(a_2,1) &
\forall l \geq 2
\end{align*}

Recall that -- up to constant terms-- the logdensity of such a Normal multivariate distribution is:
$$\log[Y|\Lambda,\Sigma] = -\frac{n}{2}\left(\log\vert C\vert +\text{Tr}(C^{-1}\hat{S}_Y)\right)$$
with

$$ C= \Lambda\Lambda'+\Sigma\\
\hat{S}_Y=\frac{1}{n}\sum_{i=1}^n Y'_iY_i$$

In the case $q<<p$, because $\Sigma$ is diagonal, it is easier to compute the inverse of the variance-covariance matrix by $$C^{-1}=\Sigma^{-1}-\Sigma^{-1}\Lambda(I+ \Lambda' \Sigma^{-1}\Lambda)^{-1}\Lambda'\Sigma^{-1}$$


Some example data are generated in the following R code chunk.

```{r generation}
source("utils_PPCA_generating_data.R") # For true values
Y <- read.table("data_PPCA.txt", sep = ";") %>%
  as.matrix()
```


# Variational approach

## Variational family

We denote $Z$ the unknown with components $(\tau,\phi,\eta,\Lambda,\Sigma)$ and choose an independent family $q(Z) = \left(\prod_u q_{z_u}(Z_u) \right)$ to approximate $[Z|Y]$ such that :
\begin{align*}
\delta_h &
\sim q_{\delta_h}(\cdot)= \mathcal{G}(A^{\delta}_h,B^{\delta}_h), &
1\leq h \leq q 
\\
\phi_{jh} &
\sim q_{\phi_{jh}}(\cdot)= \mathcal{G}(A^{\phi}_{jh},B^{\phi}_{jh}),  &
1\leq j \leq p, 1\leq h \leq q
\\
\sigma^{-2}_{j} &
\sim q_{\sigma^{-2}_{j}}(\cdot) = \mathcal{G}(A^{\sigma}_j,B^{\sigma}_j), &
1\leq j \leq p 
\\
\eta_i &
\sim q_{\eta_i}(\cdot)= \mathcal{N}(M_{\eta_i}, V^{\eta_i}), & 
1\leq i \leq n
\\
\Lambda_j &
\sim q_{\Lambda_j }(\cdot)= \mathcal{N}(M_{\lambda_j},V^{\Lambda_j}), &
1\leq j \leq p, 
\end{align*}
where $\eta_i$ denotes the $i$-th row of yhe $n\times q$ matrix $\eta$, and $\Lambda_j$ denotes the $j$-th row of the $p\times q$ matrix $\Lambda$. In the following, all vectors are treated as column vectors.

## Evidence lower bound

To update parameters, we rely on the minimisation of the Kullback Liebler divergence
$KL= \mathbb{E}_q\left[\log\frac{q(Z)}{[Z|Y]}\right]$.
Equivalently we want to maximize the ELBO

$$\text{ELBO}= \mathbb{E}_q\left[\log[Y,\tau,\phi,\eta,\Lambda,\sigma^{-2}]\right]-\mathbb{E}_q\left[\log q(\tau,\phi,\eta,\Lambda,\sigma^{-2})\right]$$

Due to conditionnal independence the criterion can decomposed as 

\begin{align*}
\text{ELBO}=&\mathbb{E}_q\left[\log\left([Y|\eta,\Lambda,\Sigma]\right)\right]\\
& + \mathbb{E}_q\left[\log\left([\Sigma]\right)\right]\\
& + \mathbb{E}_q\left[\log\left([\eta]\right) \right]\\
& + \mathbb{E}_q\left[\log\left([\Lambda|\delta,\phi]\right)\right]\\
& + \mathbb{E}_q\left[\log\left([\phi]\right)\right]\\
& + \mathbb{E}_q\left[\log\left([\delta]\right)\right]\\
& -\mathbb{E}_q\left[\log q(\Lambda,\Sigma,\eta,\phi,\delta)\right]
\end{align*}

## Terms to integrate

Up to constant terms:
\begin{align*}
\log[Y|\Lambda,\Sigma,\eta]  &
= \frac{n}{2}\sum_{j = 1}^p \log \sigma^{-2}_j - \frac{1}{2}\sum_{j = 1}^p \sigma^{-2}_j\left(Y^{j} - \eta \Lambda\right)^T\left(Y^{j} - \eta \Lambda\right)
\\
&= -\frac{n}{2}\left(\log\vert \Sigma^{-1}\vert +\text{Tr}(\Sigma^{-1}\hat{S}_{Y,\eta,\Lambda})\right),
\end{align*}
with

$$ \hat{S}_{Y,\eta,\Lambda}=\frac{1}{n}\sum_{i=1}^n (Y_i-\Lambda\eta_i')(Y'_i-\eta_i\Lambda')\\
= \frac{1}{n} (Y'-\eta\Lambda')(Y-\lambda\eta').$$


Continuing (still up ton constant terms), we have for $\eta$:
$$\log[\eta] = -\frac{1}{2}\sum_{i = 1}^n \eta_i^T\eta_i = -\frac{1}{2}\text{Tr}\left(\eta'\eta\right).$$
Since $\Sigma$ is diagonal:

$$\log[\Sigma^{-1}] = \sum_{j=1}^p\left((a_\sigma -1)\log(\sigma_j^{-2}) -b_\sigma\sigma_j^{-2}\right)$$

Due to normal assumptions for $\Lambda_j$

$$\log\left([\Lambda_{j}|\tau_{h},\phi_{j,h}]\right)= \frac{1}{2}\sum_{h = 1}^q \left(\log \phi_{j,h} + \sum_{\ell = 1}^h\log \delta_\ell\right) - \frac{1}{2}\text{Tr}\left(\text{diag}\left(\phi_{j,h}\prod_{\ell = 1}^h\delta_\ell\right)\Lambda_{j}\Lambda_{j}^T \right).$$
Remains the with the prior for $\phi_{j,h}\;(1\leq j \leq p, 1\leq h \leq q)$:

$$\log[\phi_{jh}] = \left(\frac{\nu}{ 2} - 1\right)\log(\phi_{j,h}) -\frac{\nu}{ 2}\phi_{jh},$$
and $\delta_h\; (1\leq h \leq q)$

$$
\log[\delta_{h}] = \left\lbrace 
\begin{array}{lr}
(a_1 -1)\log(\delta_{1}) -b_{1} \delta_{1} & \text{ if } h = 1,\\
(a_2 -1)\log(\delta_{2}) -b_{2} \delta_{2} & \text{ else.}
\end{array}
\right.
$$


<!-- Wrapping up all the previous log density terms -->


<!-- \begin{align*} -->
<!-- \text{ELBO}=& -\mathbb{E}_q\left[\log q(\Lambda,\Sigma,\eta,\phi,\delta)\right]\\ -->
<!-- & +\mathbb{E}_q\left[\frac{n}{2}\left(\sum_{j=1}^p \log\sigma_j^{-2} \right)-0.5\sum_{i=1}^n tr\left(\Sigma^{-1} (Y_i-\Lambda\eta_i')(Y'_i-\eta_i\Lambda')\right)\right]\\ -->
<!-- & + \mathbb{E}_q\left[\sum_{j=1}^p\left((a_\sigma -1)log(\sigma_j^{-2}) -b_\sigma\sigma_j^{-2}\right)\right]\\ -->
<!-- & + \mathbb{E}_q\left[\frac{1}{2}tr(\eta'\eta) \right]\\ -->
<!-- & + \sum_{j=1,h=1}^{p,q}\mathbb{E}_q\left[0.5log(\phi_{jh})-0.5\Lambda_{jh}^2\phi_{jh}\prod_{k=1}^h \delta_k+0.5\sum_{k=1}^h log(\delta_k) \right]\\ -->
<!-- & + \mathbb{E}_q\left[(\frac{1}{ 2} \gamma -1)log(\phi_{jh}) -\frac{\gamma}{ 2}\phi_{jh}\right]\\ -->
<!-- & + \mathbb{E}_q\left[(a_1 -1)log(\delta_{1}) - \delta_{1}\right]\\ -->
<!-- & + \mathbb{E}_q\left[(a_2 -1)log(\delta_{2}) - \delta_{2}\right]\\ -->
<!-- & + \cdots\\ -->
<!-- & + \mathbb{E}_q\left[(a_2 -1)log(\delta_{q}) - \delta_{q}\right] -->
<!-- \end{align*} -->


<!-- Note that in all cases, expectations are  analytically available. -->

## Updates for the coordinate ascent algorithm

In our context, we want to maximize the ELBO with respect to $\theta = \left\lbrace \left(M^{\Lambda_j}, V^{\Lambda_j}\right)_{1\leq j \leq p}, \left(M^{\Lambda_i}, V^{\Lambda_i}\right)_{1\leq i \leq n}, \left(A^{\sigma_j}, B^{\sigma_j}\right)_{1\leq j \leq p}, \left(A^{\phi_{j,h}}, B^{\phi_{j,h}}\right)_{1\leq j \leq p}, \left(A^{\delta_h}, B^{\delta_h}\right)_{1\leq h \leq q} \right\rbrace$.
A convenient way to maximize the ELBO in the context of conjugate mean field family is to proceed iteratively via a coordinate ascent algorithm.

Starting from an initial guess $\theta^{(0)}$, we build a sequence $\left(\theta^{(k)}\right)_{k\geq 0}$ using the following procedure:
For any unknown component $Z_u$, the variational parameters are updated by identifying the distribution given by $\mathbb{E}{q_{-u}}\left[f(Z_u,Z_{-u})\right]$, where $Z_{-u}$ denotes all the hidden random variables except $Z_u$, $\mathbb{E}_{q_{-u}}$ denotes the expectation with respect to these random variables, and $f(Z_u,Z_{-u})$ denotes all the terms in the ELBO that depend on $Z_u$ (and potentially $Z_{-u}$). 

Let's illustrate this on the random variable $\sigma^{-2}_j$:

### Updates for $\sigma^{-2}_j$

In the ELBO, we see that
$$
f_{\sigma^{-2}_j}(Z_{\sigma^{-2}_j}, Z_{-\sigma^{-2}_j}) = (a_\sigma -1)\log(\sigma_j^{-2}) -b_\sigma\sigma_j^{-2} + \frac{n}{2} \log \sigma^{-2}_j - \frac{1}{2} \sigma^{-2}_j\left(Y^{j} - \eta \Lambda_j\right)^T\left(Y^{j} - \eta \Lambda_j\right)
$$

Hence, 
$$
\mathbb{E}_{q_{-\sigma^{-2}_j}}[f_{\sigma^{-2}_j}(Z_{\sigma^{-2}_j}, Z_{-\sigma^{-2}_j})] = \left(a_\sigma + \frac{n}{2} - 1\right)\log \sigma^{-2}_j - \left(b_{\sigma} + \frac{1}{2}\mathbb{E}_{q_{\eta,\Lambda_j}}\left[\left(Y^{j} - \eta \Lambda_j\right)^T\left(Y^{j} - \eta \Lambda_j\right)\right] \right)\sigma^{-2}_j + \text{Cst}.
$$
Therefore, one can immediatly recognize the log p.d.f. of a Gamma distribution whose parameters are given by:

* $A^{\sigma_j}= a_\sigma+\frac{n}{2}$; 
* $B^{\sigma_j}_j= b_\sigma + \frac{1}{2}\mathbb{E}_{q_{\eta,\Lambda_j}}\left[\left(Y^{j} - \eta \Lambda_j\right)^T\left(Y^{j} - \eta \Lambda_j\right)\right]$. 

It remains to compute the expectation. First, note that we have:
$$\mathbb{E}\left[ \left(Y^{j} - \eta \Lambda_j\right)^T\left(Y^{j} - \eta \Lambda_j\right) \right] = (Y^{j})^TY^{j} - 2 (Y^{j})^T\mathbb{E}[\eta] \mathbb{E}[\Lambda_j] + \text{Tr}\left(\mathbb{E}[\eta^T\eta]\mathbb{E}\left[\Lambda_j \Lambda_j^T\right] \right).$$

Now, use the fact that (denoting this time $\eta_i$ the $i$-th row of $\eta$):
$$\mathbb{E}[\eta^T\eta] = \sum_{i = 1}^n\mathbb{E}\left[\eta_i\eta_i^T\right] = \sum_{i = 1}^n\left(\mathbb{E}\left[\eta_i\right]\mathbb{E}\left[\eta_i\right]^T + \mathbb{V}\left[\eta_i\right]\right).$$
Suppose we are at iteration $k$ of the algorithm, and therefore that out current guess is $\theta^{(k)}$., we then have that:
\begin{align*}
A^{\sigma_j} &
= a_\sigma+\frac{n}{2}
\\
B^{\sigma_j}_j &
= b_\sigma + \frac{1}{2}\left[(Y^{j})^TY^{j} - 2 (Y^{j})^TM^\eta M^{\Lambda_j} + \text{Tr}\left( \left(M^{\eta_i}\left(M^{\eta_i} \right)^T + V^{\eta_i}\right) \times \left(M^{\Lambda_j}\left(M^{\Lambda_j}\right)^T + V^{\Lambda_j}\right) \right)\right],
\end{align*}
where all the parameters are the one at the current iteration (we omit the dependance on $(k)$), and $M^\eta$ is the matrix stacking row-wise all the $M^{\eta_i}$.

**Remark on the computation** Note that the computation requires to compute the trace of a matrix product. 
A computational trick here to avoid potentially big matrix product is to notice that when the 2 involved matrices are symmetric (which is the case here), the trace of the matrix product is equal to the sum of all the elements of the matrix given by the element-wise product (also called the Hadamard product). Formally, for symmetric $n\times n$ matrices  $A$ and $B$:
$$\text{Tr}\left({AB}\right) = \sum_{i , j = 1}^n (A\odot B)_{i,j}$$

This trick can lead to critical reduction in computation time (a factor 20 for a 100$\times$ 100 matrix, in `R`).



 * $A^{\phi}_{jh}= \frac{\gamma}{2}+\frac{1}{2}$; 
 * $B^{\phi}_{jh}=\frac{\gamma}{2}+\frac{1}{2}\mathbb{E}_q (\Lambda_{jh} ^2 ) \prod_{k=1}^h \mathbb{E}_q (\delta_k)$; 
 
 * $A^{\delta}_1= a_1+\frac{p}{2}$; 
 * $B^{\delta}_{1}=1+\frac{1}{2}\sum_{l=1}^q \sum_{j=1}^p \mathbb{E}_q (\Lambda_{jl} ^2 )\mathbb{E}_q (\phi_{jl} ) \prod_{t=2}^l \mathbb{E}_q (\delta_t)$;  
 
 for $l \ge 2$: 
 
 * $A^{\delta}_h= a_2+\frac{p(q-h+1)}{2}$; 
 * $B^{\delta}_{h}=1+\frac{1}{2}\sum_{l=h}^q \sum_{j=1}^p \mathbb{E}_q (\Lambda_{jl} ^2 )\mathbb{E}_q (\phi_{jl} ) \prod_{t=1, t\ne h}^l \mathbb{E}_q (\delta_t)$;  

 
 The variational normal distribution for $\eta_i$ is obtained as follows:
 
 * First, let's compute the precision first component, $\tilde{P}^{\eta_i}=\sum_{j=1}^p \frac{A^{\sigma_j}}{B^{\sigma_j}}V^{\lambda_j}$
 * Then the precision, $(V^{\eta_i})^{-1}=\tilde{P}^{\eta_i}+I_{qq}$
 
 * And subsequently the variational mean
 $M^{\eta_i}=V^{\eta_i}\mathbb{E}_q (\Lambda')\mathbb{E}_q ( \Sigma^{-1})Y_i$
 
 The variational normal distributions for $\lambda_j$ is obtained as follows:
 
 * ${P}^{\lambda_j}=\mathbb{E}_q({\sigma_j^{-2}}\eta'\eta)+\mathbb{E}_q({D^{-1}}_\eta'\eta_j)$
 
 * The precision first component, $\tilde{P}^{\lambda_j}=\left(\frac{A^{\sigma_j}}{B^{\sigma_j}}\right)\sum_{i=1}^n (V^{\eta_i}+{M^{\eta_i}}{M^{\eta_i}}')$ . Note that the matrix $\sum_{i=1}^n (V^{\eta_i}+{M^{\eta_i}}{M^{\eta_i}}')$ is to be computed once for all $1\le j \le p)$
 
 * Then the precision, $(V^{\lambda_j})^{-1}=\tilde{P}^{\lambda_j}+diag(\frac{A^{\phi_{jh}}}{B^{\phi_{jh}}} \prod_{l=1}^h\frac{A^{\delta_{l}}}{B^{\delta_{l}}} )$
 
 * And subsequently the variational mean
 $M^{\lambda_j}=V^{\lambda_j}\left(\frac{A^{\sigma_j}}{B^{\sigma_j}}\right) {M^{\eta}}'Y^{j}$
 
 These update equations are implemented within functions.
 
We can check the evolution of the ELBO across iterations. To compute the ELBO, one needs to recall that

 * the entropy of a Normal ($\mu,\Sigma$) distribution is $\frac{log(\vert \Sigma \vert)}{2}+ \frac{dim(\Sigma)}{2} (1+log(2\pi))$;
 
 * the entropy of a Gamma ($\alpha,\beta$) distribution is $\alpha-log(\beta)+log\Gamma(\alpha)+(1-\alpha) \psi(\alpha)$ with $\psi(.)$ being the so-call digamma function;
 
 * from the previous computations, we see that $\mathbb{E}_q($
 $2(B^{\sigma}_j- b_\sigma)= \mathbb{E}_q ((Y_{ij}-\Lambda'_j\eta_i)^2)$;
 
 * and that $\mathbb{E}_q($
 $2(B^{\phi}_{jh}- 0.5 \gamma)= \mathbb{E}_q (\Lambda^2 \prod_{k=1}^q \delta_k)$.

Hence: 
\begin{align*}
ELBO=& -\mathbb{E}_q\left[\log q(\Lambda,\Sigma,\eta,\phi,\delta)\right]\\
& +\frac{n}{2}\sum_{j=1}^p\left( \psi(A_j^{\sigma})-log(B_j^{\sigma}) \right)\\
& -\left[ \sum_{j=1}^p \frac{A^{\sigma}_j}{B^{\sigma}_j} (B^{\sigma}_j- b_\sigma)\right]\\
& + \sum_{j=1}^p\left((a_\sigma -1)\left( \psi(A_j^{\sigma})-log(B_j^{\sigma}) \right) -b_\sigma\frac{A_j^{\sigma}} {B_j^{\sigma}}\right)\\
& + \left[\frac{1}{2}tr(\sum_i^p M^{\eta_i}M'^{\eta_i}+V^{\eta_i}) \right]\\
& + \sum_{j=1,h=1}^{p,q}\left[-(B^{\phi}_{jh}- 0.5 \gamma)\frac{A_{jh}^{\phi}} {B_{jh}^{\phi}}+0.5\sum_{k=1}^h \left( \psi(A_k^{\delta})-log(B_k^{\delta}) \right) \right]\\
& + \sum_{j=1,h=1}^{p,q}\left[(\frac{1}{ 2} \gamma -\frac{1}{ 2})\left( \psi(A_{jh}^{\phi})-log(B_{jh}^{\phi}) \right) -\frac{\gamma}{ 2}\frac{A_{jh}^{\phi}} {B_{jh}^{\phi}}\right]\\
& +  (a_1 -1)\left( \psi(A_1^{\delta})-log(B_1^{\delta}) \right) -\frac{A_1^{\delta}} {B_1^{\delta}}\\
& +  (a_2 -1)\left( \psi(A_2^{\delta})-log(B_2^{\delta}) \right) -\frac{A_2^{\delta}} {B_2^{\delta_q}}\\
& + \cdots\\
& + (a_2 -1)\left( \psi(A_q^{\delta})-log(B_q^{\delta}) \right) -\frac{A_q^{\delta}} {B_q^{\delta}}
\end{align*}
 
 The entropic term $-\mathbb{E}_q\left[\log q(\Lambda,\Sigma,\eta,\phi,\delta)\right]$ is straightfully computed. Up to constant terms,
 
\begin{align*}
& -\mathbb{E}_q\left[\log q(\Lambda,\Sigma,\eta,\phi,\delta)\right]= \\
& +\frac{1}{2}\sum_{j=1}^p log(\vert V^{\lambda_j} \vert)\\
& +\frac{1}{2}\sum_{i=1}^n log(\vert V^{\eta_i} \vert)\\
& +\sum_{j,h}^{p,q}\left(A_{j,h}^\phi-log(B_{j,h}^\phi)+log\Gamma(A_{j,h}^\phi)+(1-A_{j,h}^\phi) \psi(A_{j,h}^\phi) \right)\\
& +\sum_{j}^{p}\left(A_j^\sigma-log(B_j^\sigma)+log\Gamma(A_j^\sigma)+(1-A_j^\sigma) \psi(A_j^\sigma) \right)\\
&  +\sum_{h}^{q}\left(A_{h}^\delta-log(B_{h}^\delta)+log\Gamma(A_{h}^\delta)+(1-A_{h}^\delta) \psi(A_{h}^\delta) \right)
\end{align*}

Before exploring results, we will perform the inference on the same model with MCMC (in `Jags`).

# Comparaison with MCMC inference

Relying on Jags, we can get MCMC samples from the posterior distribution of the unknows.

```{r modelString}
modelString = "
model{
#a1 <- 2
#a2 <- 3
a[1] ~ dgamma(2,1)T(2,)
a[2] ~ dgamma(2,1)T(3,)
deminu <- 3/2
a_sigma <- 0.1
b_sigma <- 0.1

#shrinkage prior
for( h in 2:q){delta[h] ~ dgamma(a[2],1)}
              delta[1] ~ dgamma(a[1],1)
for( h in 1:q){ tau[h] <- prod(delta[1:h])} 
for( j in 1:p){
               for( h in 1:q){
                              phi[j,h] ~ dgamma(deminu,deminu)
                              precilambda[j,h] <- phi[j,h]*tau[h]
                              lambda[j,h]~ dnorm(0,precilambda[j,h])
                              }
               }
# N(0,1) prior for eta
for(i in 1:n){
              for (h in 1:q){
                      eta[i,h] ~ dnorm(0,1)
                            }
}
muY <- eta %*% t(lambda)
# finally Y
for(j in 1:p){ preciE[j] ~ dgamma(a_sigma,b_sigma)
              for (i in 1:n){
                      Y[i,j] ~ dnorm(muY[i,j],preciE[j])
                            }
}

}"
```


```{r jags_inference, eval = FALSE}
data_for_JAGS <- list(
  n = dim(Y)[1],
  p = dim(Y)[2],
  q = 6,
  Y = Y
)
n.adapt = 1000
burnin = 1000
n.iter = burnin * 2
thin = 1
jm <- jags.model(
  file = textConnection(modelString),
  data = data_for_JAGS,
  n.chains = 3 ,
  n.adapt = n.adapt
)
update(jm, burnin)
jsamples <- coda.samples(
  model = jm,
  variable.names = c("tau","a"),
  n.iter = n.iter,
  thin = thin
)
```


We transform the MCMC coda file into a regular tibble to visualize posterior pdfs and variational ones.

```{r MCMCvisualization}
MCMC_results <- ggs(jsamples)
ggs_traceplot(MCMC_results)
ggs_density(MCMC_results %>%
              filter(Parameter %in% c("a[1]","a[2]")))
ggs_density(MCMC_results,family=c("tau"))
ggs_caterpillar(MCMC_results,family = "tau") +
  scale_x_continuous(trans = "log10")
```

We can now visualize the estimated posterior distributions for all parameters:


